{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX_As7WC513m"
      },
      "source": [
        "### Implementing Vision Transformer from scratch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gL28j-S6rKU"
      },
      "outputs": [],
      "source": [
        "#Lets import the necessary libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRnPh976EQca"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "IMG_SIZE = 32 # CIFFAR image resolution (32 x 32)\n",
        "PATCH_SIZE = 8 # Each patch will be 8 x 8 pixels\n",
        "IN_CHANNELS = 3 #RGB\n",
        "EMBED_DIM = 64\n",
        "NUM_HEADS = 4\n",
        "DEPTH = 4 # Number of transformer encoder blocks\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM1kUpS77YTz"
      },
      "source": [
        "### Patch Embedding\n",
        "\n",
        "The first task in Vit is to convert the images into patches.\n",
        "Lets create a patch embedding layer that converts the images into patches then to vector embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1TrkICdC-6T"
      },
      "source": [
        "**NOTE: why we are using conv2d?**\n",
        "\n",
        "\n",
        "1. Task One: The \"Cutter\" (Splitting into Patches) ‚úÇÔ∏è\n",
        "\n",
        "The `Conv2d` layer replaces the need for a manual loop to chop up the image. It uses the Stride and Kernel Size to determine how the image is divided.\n",
        "\n",
        "‚Ä¢ The Logic: By setting `stride = kernel_size`, we force the filter to jump exactly the width of one patch after processing it. This ensures no overlap between patches.\n",
        "\n",
        "‚Ä¢ Example:\n",
        "\n",
        "  ‚Ä¢ Image Size: 16x16 pixels.\n",
        "\n",
        "  ‚Ä¢ Kernel Size: 4 (This defines the patch size as 4x4).\n",
        "\n",
        "  ‚Ä¢ Stride: 4 (The jump size).\n",
        "\n",
        "The Process:\n",
        "\n",
        "1. The filter lands on the first 4x4 block (top-left).\n",
        "\n",
        "2. It processes it.\n",
        "\n",
        "3. It jumps exactly 4 pixels to the right.\n",
        "\n",
        "4. It lands perfectly on the next 4x4 block, skipping nothing and overlapping nothing.\n",
        "\n",
        "Result: You get a grid of 16 patches (4 rows x 4 columns).\n",
        "\n",
        "2. Task Two: The \"Translator\" (Creating Embeddings) üó£Ô∏è\n",
        "\n",
        "We don't just want pixel grids; we want vectors (lists of numbers) that represent the content of those patches.\n",
        "\n",
        "We define an Embedding Dimension (e.g., `embed_dim = 64`). This means we want every 4x4 patch to be summarized by exactly 64 numbers.\n",
        "\n",
        "To do this, the `Conv2d` layer creates 64 separate filters. Each filter is a unique \"feature detector.\"\n",
        "\n",
        "‚Ä¢ The Filter Shape: Since your patch is 4x4 with 3 color channels (RGB), every single filter has the shape 4x4x3.\n",
        "\n",
        "How the Embedding is built for ONE patch:\n",
        "\n",
        "‚Ä¢ Filter 1: A block of weights (4x4x3). It overlays the patch, multiplies all the pixel values by its weights, sums them up, and produces 1 single number.\n",
        "\n",
        "‚Ä¢ Filter 2: A different block of weights (4x4x3). It looks at the same patch and produces a 2nd number.\n",
        "\n",
        "‚Ä¢ ... (repeating this process) ...\n",
        "\n",
        "‚Ä¢ Filter 64: It looks at the patch and produces the 64th number.\n",
        "\n",
        "The Result: For that single patch, you now have a stack of 64 numbers. That is your Patch Embedding Vector. üíé\n",
        "\n",
        "3. The Grand Finale (The Output Shape) üèÅ\n",
        "\n",
        "After the `Conv2d` operation finishes running over your 16x16 image:\n",
        "\n",
        "1. Patches Created: 16 patches (arranged in a 4x4 grid).\n",
        "\n",
        "2. Embedding per Patch: 64 numbers.\n",
        "\n",
        "3. Final Output Shape: `(Batch_Size, 64, 4, 4)`.\n",
        "\n",
        "This tensor contains 16 patches, where each patch is now represented by a deep vector of 64 features instead of raw pixels!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LehaJXJq66a9"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "\n",
        "  # for example, lets lake 16 * 16 pixel image and patch length as 4\n",
        "  # img_size = 16 , patch_size = 4, channels = 3 (RGB channels)\n",
        "  # embed_dim is size of the vector we want to transform the patches\n",
        "    def __init__(self, img_size= IMG_SIZE,\n",
        "                 patch_size = PATCH_SIZE,\n",
        "                 in_channels = IN_CHANNELS,\n",
        "                 embed_dim = EMBED_DIM):\n",
        "\n",
        "      super().__init__()\n",
        "      self.patch_size = patch_size\n",
        "\n",
        "\n",
        "      # splitting the image into patches\n",
        "      # we can split the img into patches using manual slicing\n",
        "      # but convolution is more simpler shortcut\n",
        "      self.projection = nn.Conv2d(in_channels, embed_dim,\n",
        "                                  kernel_size = patch_size,\n",
        "                                  stride = patch_size)\n",
        "\n",
        "      self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      # before projection x.shape = [B, 1, 16, 16]\n",
        "      x = self.projection(x)\n",
        "\n",
        "      #after projection x.shape = [B, embed_dim, 4, 4]\n",
        "\n",
        "      x = x.flatten(2)\n",
        "\n",
        "      #after flatten x.shape = [B, embed_dim, 16]\n",
        "\n",
        "      x = x.transpose(1, 2)\n",
        "\n",
        "      #after transpose x.shape = [B,16, embed_dim]\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JWga6vkFPux"
      },
      "source": [
        "### Encoder block implementation\n",
        "\n",
        "Each encoder block contains layer normalization, multi lead attention and a small feedforward network connected using residual connection\n",
        "\n",
        "\n",
        "\n",
        "The encoder block is pretty much straight forward like in original vision transformer paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIMmwgBkaRzI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # The dimension of each individual attention head\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Ensure the embedding dimension can be evenly divided by the number of heads\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        # The scaling factor (1 / sqrt(d_k)) to prevent dot products from getting too large\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        # A single linear layer to calculate Q, K, and V simultaneously (3 * embed_dim)\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=True)\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "\n",
        "        # The final linear projection to mix the concatenated heads back together\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # B = Batch Size\n",
        "        # N = Sequence Length (Number of patches + 1 for CLS token)\n",
        "        # C = Embedding Dimension\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # 1. Project input to Q, K, V\n",
        "        # Resulting shape: (B, N, 3 * C)\n",
        "        qkv = self.qkv(x)\n",
        "\n",
        "        # 2. Reshape and Permute to separate the heads\n",
        "        # Step A: Reshape to (B, N, 3, num_heads, head_dim)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Step B: Permute to (3, B, num_heads, N, head_dim)\n",
        "        # We bring the '3' to the front to easily unpack Q, K, and V\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        # Unpack the tensor into our Query, Key, and Value tensors\n",
        "        # Each has shape: (B, num_heads, N, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # 3. Calculate Attention Scores (Q * K^T)\n",
        "        # We transpose the last two dimensions of K to do the matrix multiplication\n",
        "        # Resulting shape: (B, num_heads, N, N)\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # 4. Apply Softmax to get attention probabilities\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # 5. Multiply Attention Scores by Values (attn * V)\n",
        "        # Resulting shape: (B, num_heads, N, head_dim)\n",
        "        x = attn @ v\n",
        "\n",
        "        # 6. Concatenate the heads back together\n",
        "        # We transpose back to (B, N, num_heads, head_dim) and then flatten the last two dims\n",
        "        # Resulting shape: (B, N, C)\n",
        "        x = x.transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "        # 7. Final linear projection and dropout\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9YeFGGOEKGH"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "  # mlp_ratio = 4, standard in transformers\n",
        "  # mlp is usuallu 4Xlarger thah the input layer\n",
        "  def __init__(self, embed_dim = EMBED_DIM,\n",
        "               num_heads = NUM_HEADS,\n",
        "               mlp_ratio = 4.0,\n",
        "               dropout = 0.0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.attn = MultiHeadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "\n",
        "\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    hidden_features = int(embed_dim * mlp_ratio)\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embed_dim, hidden_features),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_features, embed_dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    attn_input = self.norm1(x)\n",
        "\n",
        "    attn_output = self.attn(attn_input)\n",
        "    x = x + attn_output\n",
        "\n",
        "    mlp_input = self.norm2(x)\n",
        "\n",
        "    mlp_output = self.mlp(mlp_input)\n",
        "\n",
        "    x = x + mlp_output\n",
        "\n",
        "    return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ491FgEHCfC"
      },
      "source": [
        "### Vision Transformer model\n",
        "\n",
        "Now we assemble all the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzwGALq0G05t"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size = IMG_SIZE,\n",
        "               patch_size = PATCH_SIZE,\n",
        "               num_classes = NUM_CLASSES,\n",
        "               embed_dim = EMBED_DIM,\n",
        "               depth = DEPTH,\n",
        "               num_heads = NUM_HEADS,\n",
        "               in_channels =  IN_CHANNELS,\n",
        "               dropout = 0.1):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "\n",
        "    num_patches = self.patch_embedding.num_patches\n",
        "\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    self.pos_embedding = nn.Parameter(torch.zeros(1, 1 + num_patches, embed_dim))\n",
        "    nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
        "\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    self.encoder = nn.Sequential(*[EncoderBlock(embed_dim, num_heads, dropout=dropout) for _ in range(depth)])\n",
        "\n",
        "\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.Linear(embed_dim, num_classes)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "        # Extract patches and project\n",
        "        x = self.patch_embedding(x)\n",
        "        B = x.size(0)\n",
        "\n",
        "        # Prepend the CLS token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embedding\n",
        "\n",
        "        # Apply dropout after positional embedding\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # 6. Streamlined Encoder pass\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # Extract the CLS token output (the 0th index)\n",
        "        x = x[:, 0]\n",
        "\n",
        "        # Apply the final norm and pass through the classification head\n",
        "        x = self.norm(x)\n",
        "        out = self.mlp_head(x)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzENjY-BI1W6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u1qLYmWNluL",
        "outputId": "a5b6825e-2f5e-48a8-9c2e-b0ce4d6176f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:03<00:00, 44.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- DATA PREP ---\n",
        "train_transform = transforms.Compose([\n",
        "transforms.RandomCrop(32, padding=4),\n",
        "transforms.RandomHorizontalFlip(),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), # CIFAR-10 standard normalization\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-fU8tT4OLcP"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AreMZt1N3-f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_get_acc(embed_dim, num_heads, depth, epochs):\n",
        "    print(f\"Training: Dim={embed_dim}, Heads={num_heads}, Depth={depth}\")\n",
        "\n",
        "    # Instantiate VisionTransformer with the provided parameters\n",
        "    model = VisionTransformer(embed_dim=embed_dim, num_heads=num_heads, depth=depth).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    acc_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct_epoch = 0\n",
        "        total_epoch = 0\n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct = (preds == labels).sum().item()\n",
        "\n",
        "            correct_epoch += correct\n",
        "            total_epoch += labels.size(0)\n",
        "\n",
        "\n",
        "        # End of Epoch Training Summary\n",
        "        epoch_train_acc = 100.0 * correct_epoch / total_epoch\n",
        "        print(f\"==> Epoch {epoch+1} Summary: Total Loss = {total_loss:.4f}, Training Accuracy = {epoch_train_acc:.2f}%\")\n",
        "\n",
        "        # Validation after each epoch\n",
        "        model.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in testloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                correct_test += (preds == labels).sum().item()\n",
        "                total_test += labels.size(0)\n",
        "\n",
        "        test_acc = 100.0 * correct_test / total_test\n",
        "        acc_history.append(test_acc)\n",
        "        print(f\"==> Test Accuracy after Epoch {epoch+1}: {test_acc:.2f}%\")\n",
        "\n",
        "    return acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6fI__yrN_Ql",
        "outputId": "8200d072-eef0-494c-ec55-fb36013c72ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: Dim=64, Heads=4, Depth=4\n",
            "\n",
            "Epoch 1\n",
            "==> Epoch 1 Summary: Total Loss = 1456.4128, Training Accuracy = 29.80%\n",
            "==> Test Accuracy after Epoch 1: 34.15%\n",
            "\n",
            "Epoch 2\n",
            "==> Epoch 2 Summary: Total Loss = 1318.9920, Training Accuracy = 37.25%\n",
            "==> Test Accuracy after Epoch 2: 39.37%\n",
            "\n",
            "Epoch 3\n",
            "==> Epoch 3 Summary: Total Loss = 1241.0304, Training Accuracy = 41.40%\n",
            "==> Test Accuracy after Epoch 3: 45.11%\n",
            "\n",
            "Epoch 4\n",
            "==> Epoch 4 Summary: Total Loss = 1187.3029, Training Accuracy = 44.35%\n",
            "==> Test Accuracy after Epoch 4: 47.38%\n",
            "\n",
            "Epoch 5\n",
            "==> Epoch 5 Summary: Total Loss = 1146.4641, Training Accuracy = 46.46%\n",
            "==> Test Accuracy after Epoch 5: 49.31%\n",
            "\n",
            "Epoch 6\n",
            "==> Epoch 6 Summary: Total Loss = 1113.6682, Training Accuracy = 47.82%\n",
            "==> Test Accuracy after Epoch 6: 50.79%\n",
            "\n",
            "Epoch 7\n",
            "==> Epoch 7 Summary: Total Loss = 1079.9252, Training Accuracy = 49.95%\n",
            "==> Test Accuracy after Epoch 7: 52.89%\n",
            "\n",
            "Epoch 8\n",
            "==> Epoch 8 Summary: Total Loss = 1051.5142, Training Accuracy = 51.18%\n",
            "==> Test Accuracy after Epoch 8: 54.10%\n",
            "\n",
            "Epoch 9\n",
            "==> Epoch 9 Summary: Total Loss = 1025.6796, Training Accuracy = 52.70%\n",
            "==> Test Accuracy after Epoch 9: 53.43%\n",
            "\n",
            "Epoch 10\n",
            "==> Epoch 10 Summary: Total Loss = 1006.8634, Training Accuracy = 53.30%\n",
            "==> Test Accuracy after Epoch 10: 55.77%\n",
            "\n",
            "Epoch 11\n",
            "==> Epoch 11 Summary: Total Loss = 986.0112, Training Accuracy = 54.30%\n",
            "==> Test Accuracy after Epoch 11: 56.46%\n",
            "\n",
            "Epoch 12\n",
            "==> Epoch 12 Summary: Total Loss = 967.5442, Training Accuracy = 55.06%\n",
            "==> Test Accuracy after Epoch 12: 58.42%\n",
            "\n",
            "Epoch 13\n",
            "==> Epoch 13 Summary: Total Loss = 950.4602, Training Accuracy = 56.05%\n",
            "==> Test Accuracy after Epoch 13: 59.89%\n",
            "\n",
            "Epoch 14\n",
            "==> Epoch 14 Summary: Total Loss = 942.5059, Training Accuracy = 56.46%\n",
            "==> Test Accuracy after Epoch 14: 58.80%\n",
            "\n",
            "Epoch 15\n",
            "==> Epoch 15 Summary: Total Loss = 932.5788, Training Accuracy = 56.91%\n",
            "==> Test Accuracy after Epoch 15: 59.76%\n",
            "\n",
            "Epoch 16\n",
            "==> Epoch 16 Summary: Total Loss = 915.3722, Training Accuracy = 57.74%\n",
            "==> Test Accuracy after Epoch 16: 60.94%\n",
            "\n",
            "Epoch 17\n",
            "==> Epoch 17 Summary: Total Loss = 906.8210, Training Accuracy = 58.34%\n",
            "==> Test Accuracy after Epoch 17: 60.71%\n",
            "\n",
            "Epoch 18\n",
            "==> Epoch 18 Summary: Total Loss = 895.8245, Training Accuracy = 58.73%\n",
            "==> Test Accuracy after Epoch 18: 60.84%\n",
            "\n",
            "Epoch 19\n",
            "==> Epoch 19 Summary: Total Loss = 890.9744, Training Accuracy = 59.04%\n",
            "==> Test Accuracy after Epoch 19: 61.57%\n",
            "\n",
            "Epoch 20\n",
            "==> Epoch 20 Summary: Total Loss = 886.1292, Training Accuracy = 59.15%\n",
            "==> Test Accuracy after Epoch 20: 62.35%\n",
            "\n",
            "Epoch 21\n",
            "==> Epoch 21 Summary: Total Loss = 876.2084, Training Accuracy = 59.64%\n",
            "==> Test Accuracy after Epoch 21: 62.71%\n",
            "\n",
            "Epoch 22\n",
            "==> Epoch 22 Summary: Total Loss = 866.3095, Training Accuracy = 60.12%\n",
            "==> Test Accuracy after Epoch 22: 63.32%\n",
            "\n",
            "Epoch 23\n",
            "==> Epoch 23 Summary: Total Loss = 857.1159, Training Accuracy = 60.49%\n",
            "==> Test Accuracy after Epoch 23: 62.88%\n",
            "\n",
            "Epoch 24\n",
            "==> Epoch 24 Summary: Total Loss = 851.4940, Training Accuracy = 60.93%\n",
            "==> Test Accuracy after Epoch 24: 63.10%\n",
            "\n",
            "Epoch 25\n",
            "==> Epoch 25 Summary: Total Loss = 841.9583, Training Accuracy = 61.00%\n",
            "==> Test Accuracy after Epoch 25: 63.90%\n",
            "\n",
            "Epoch 26\n",
            "==> Epoch 26 Summary: Total Loss = 835.7562, Training Accuracy = 61.70%\n",
            "==> Test Accuracy after Epoch 26: 63.90%\n",
            "\n",
            "Epoch 27\n",
            "==> Epoch 27 Summary: Total Loss = 833.1056, Training Accuracy = 61.72%\n",
            "==> Test Accuracy after Epoch 27: 65.11%\n",
            "\n",
            "Epoch 28\n",
            "==> Epoch 28 Summary: Total Loss = 825.9602, Training Accuracy = 62.06%\n",
            "==> Test Accuracy after Epoch 28: 65.45%\n",
            "\n",
            "Epoch 29\n",
            "==> Epoch 29 Summary: Total Loss = 824.3682, Training Accuracy = 62.38%\n",
            "==> Test Accuracy after Epoch 29: 64.76%\n",
            "\n",
            "Epoch 30\n",
            "==> Epoch 30 Summary: Total Loss = 818.7229, Training Accuracy = 62.39%\n",
            "==> Test Accuracy after Epoch 30: 65.07%\n",
            "\n",
            "Epoch 31\n",
            "==> Epoch 31 Summary: Total Loss = 809.8168, Training Accuracy = 62.69%\n",
            "==> Test Accuracy after Epoch 31: 65.06%\n",
            "\n",
            "Epoch 32\n",
            "==> Epoch 32 Summary: Total Loss = 807.8829, Training Accuracy = 62.76%\n",
            "==> Test Accuracy after Epoch 32: 65.25%\n",
            "\n",
            "Epoch 33\n",
            "==> Epoch 33 Summary: Total Loss = 802.6012, Training Accuracy = 63.00%\n",
            "==> Test Accuracy after Epoch 33: 66.72%\n",
            "\n",
            "Epoch 34\n",
            "==> Epoch 34 Summary: Total Loss = 794.2434, Training Accuracy = 63.44%\n",
            "==> Test Accuracy after Epoch 34: 66.27%\n",
            "\n",
            "Epoch 35\n",
            "==> Epoch 35 Summary: Total Loss = 795.2462, Training Accuracy = 63.60%\n",
            "==> Test Accuracy after Epoch 35: 67.45%\n",
            "\n",
            "Epoch 36\n",
            "==> Epoch 36 Summary: Total Loss = 785.8379, Training Accuracy = 63.93%\n",
            "==> Test Accuracy after Epoch 36: 67.37%\n",
            "\n",
            "Epoch 37\n",
            "==> Epoch 37 Summary: Total Loss = 785.2453, Training Accuracy = 64.10%\n",
            "==> Test Accuracy after Epoch 37: 66.17%\n",
            "\n",
            "Epoch 38\n",
            "==> Epoch 38 Summary: Total Loss = 780.3946, Training Accuracy = 64.18%\n",
            "==> Test Accuracy after Epoch 38: 67.82%\n",
            "\n",
            "Epoch 39\n",
            "==> Epoch 39 Summary: Total Loss = 778.7282, Training Accuracy = 64.39%\n",
            "==> Test Accuracy after Epoch 39: 66.94%\n",
            "\n",
            "Epoch 40\n",
            "==> Epoch 40 Summary: Total Loss = 775.7084, Training Accuracy = 64.34%\n",
            "==> Test Accuracy after Epoch 40: 67.29%\n",
            "\n",
            "Epoch 41\n",
            "==> Epoch 41 Summary: Total Loss = 772.2735, Training Accuracy = 64.69%\n",
            "==> Test Accuracy after Epoch 41: 67.08%\n",
            "\n",
            "Epoch 42\n",
            "==> Epoch 42 Summary: Total Loss = 769.9450, Training Accuracy = 64.94%\n",
            "==> Test Accuracy after Epoch 42: 67.26%\n",
            "\n",
            "Epoch 43\n",
            "==> Epoch 43 Summary: Total Loss = 762.8151, Training Accuracy = 64.99%\n",
            "==> Test Accuracy after Epoch 43: 67.85%\n",
            "\n",
            "Epoch 44\n",
            "==> Epoch 44 Summary: Total Loss = 762.6798, Training Accuracy = 65.32%\n",
            "==> Test Accuracy after Epoch 44: 67.66%\n",
            "\n",
            "Epoch 45\n",
            "==> Epoch 45 Summary: Total Loss = 760.0769, Training Accuracy = 65.17%\n",
            "==> Test Accuracy after Epoch 45: 68.27%\n",
            "\n",
            "Epoch 46\n",
            "==> Epoch 46 Summary: Total Loss = 754.2909, Training Accuracy = 65.27%\n",
            "==> Test Accuracy after Epoch 46: 69.06%\n",
            "\n",
            "Epoch 47\n",
            "==> Epoch 47 Summary: Total Loss = 757.6578, Training Accuracy = 65.36%\n",
            "==> Test Accuracy after Epoch 47: 67.78%\n",
            "\n",
            "Epoch 48\n",
            "==> Epoch 48 Summary: Total Loss = 751.4114, Training Accuracy = 65.55%\n",
            "==> Test Accuracy after Epoch 48: 68.48%\n",
            "\n",
            "Epoch 49\n",
            "==> Epoch 49 Summary: Total Loss = 745.6372, Training Accuracy = 65.93%\n",
            "==> Test Accuracy after Epoch 49: 68.38%\n",
            "\n",
            "Epoch 50\n",
            "==> Epoch 50 Summary: Total Loss = 744.9526, Training Accuracy = 65.84%\n",
            "==> Test Accuracy after Epoch 50: 68.91%\n"
          ]
        }
      ],
      "source": [
        "# --- RUN EXPERIMENTS ---\n",
        "# 1. Val Accuracy vs Epochs\n",
        "acc_vs_epoch = train_and_get_acc(embed_dim=64, num_heads=4, depth=4, epochs=50)\n",
        "\n",
        "# 2. Val Accuracy vs Heads (Fixed Epochs=15, Dim=64)\n",
        "#heads_list = [2,8,16]\n",
        "#acc_vs_heads = [train_and_get_acc(64, h, h, 15)[-1] for h in heads_list]\n",
        "\n",
        "# 3. Val Accuracy vs Embed Dim (Fixed Epochs=15, Heads=4)\n",
        "#dims_list = [16, 32, 128]\n",
        "#acc_vs_dims = [train_and_get_acc(d, 4, 4, 15)[-1] for d in dims_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXpdQQxXVoLW"
      },
      "outputs": [],
      "source": [
        "# Got around 69% accuracy for the following parameters : embed_dim=64, num_heads=4, depth=4, epochs=50\n",
        "# Play with uncommenting the Val_accuray vs heads, val_accuracy vs embed dim to find the optimal parameters."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}